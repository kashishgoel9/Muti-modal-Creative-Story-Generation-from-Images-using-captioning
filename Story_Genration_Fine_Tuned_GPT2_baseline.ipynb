{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxuXEqfGqoAU"
      },
      "source": [
        "Fine Tune GPT-2 model for Creative Story Generation using Writing Prompt Dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm_9kZqnqoAa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import argparse\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I use the dataset of writing prompts and stories from https://github.com/pytorch/fairseq/tree/master/examples/stories to fine-tune GPT-2, then use the fine-tuned model to generate stories."
      ],
      "metadata": {
        "id": "ugfOSPFy9oe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7rqie2rqoAd"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip install transformers/\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers.optimization import AdamW, get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPa0joTkqoAg"
      },
      "outputs": [],
      "source": [
        "# Arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--seed', type=int, default=88888)\n",
        "parser.add_argument(\"--model_name\", default=\"gpt2\", type=str)\n",
        "parser.add_argument(\"--max_seq_length\", default=512, type=int)\n",
        "parser.add_argument(\"--train_batch_size\", default=4, type=int)\n",
        "parser.add_argument(\"--valid_batch_size\", default=4, type=int)\n",
        "parser.add_argument(\"--num_train_epochs\", default=1, type=int)\n",
        "parser.add_argument(\"--warmup\", default=0.1, type=float)\n",
        "parser.add_argument(\"--learning_rate\", default=5e-5, type=float)\n",
        "parser.add_argument(\"--input_text_path\", default='../input/story-text', type=str)\n",
        "args, _ = parser.parse_known_args()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Writing Prompt Dataset from https://www.kaggle.com/datasets/ratthachat/writing-prompts\n",
        "# It is already divided into train, validation and test sets. But the prompts and the stories are in the seperate files."
      ],
      "metadata": {
        "id": "GD2d9swvDMar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The train dataset is very large.\n",
        "# For the training purpose, valid dataset is taken as train dataset, and the test dataset as valid dataset.\n",
        "\n",
        "# Every line in the combined file includes the prompt and it's corresponding story concatenated together as: 'prompt + <sep> + story' for the input to the GPT-2 model."
      ],
      "metadata": {
        "id": "7AkIk1PEDj-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96NRBNTyqvfv"
      },
      "outputs": [],
      "source": [
        "# Function to combine prompts and stories\n",
        "def combinetext(prompt, story):\n",
        "    prompts = open(prompt, 'r', encoding='utf8').readlines()\n",
        "    stories = open(story, 'r', encoding='utf8').readlines()\n",
        "    assert len(prompts) == len(stories)\n",
        "    combine = []\n",
        "    for i in range(len(prompts)):\n",
        "        combine.append(prompts[i].rstrip() + ' <sep> ' + \" \".join(stories[i].split()[:300]))\n",
        "    return combine\n",
        "\n",
        "# Prprocessing the data (punctuations, etc)\n",
        "def cleanpunctuation(s):\n",
        "    for p in '!,.:;?':\n",
        "        s = s.replace(' ' + p, p)\n",
        "    s = s.replace(' ' + 'n\\'t', 'n\\'t')\n",
        "    s = s.replace(' ' + '\\'s', '\\'s')\n",
        "    s = s.replace(' ' + '\\'re', '\\'re')\n",
        "    s = s.replace(' ' + '\\'ve', '\\'ve')\n",
        "    s = s.replace(' ' + '\\'ll', '\\'ll')\n",
        "    s = s.replace(' ' + '\\'am', '\\'am')\n",
        "    s = s.replace(' ' + '\\'m', '\\'m')\n",
        "    s = s.replace(' ' + '\\' m', '\\'m')\n",
        "    s = s.replace(' ' + '\\'m', '\\'m')\n",
        "    s = s.replace(' ' + '\\' ve', '\\'ve')\n",
        "    s = s.replace(' ' + '\\' s', '\\'s')\n",
        "    s = s.replace('<newline>', '\\n')\n",
        "    return s\n",
        "\n",
        "# Combine and clean text for train and valid datasets\n",
        "train_text = combinetext('valid.wp_source', 'valid.wp_target')\n",
        "train_text = list(map(cleanpunctuation, train_text))\n",
        "\n",
        "valid_text = combinetext('test.wp_source', 'test.wp_target')\n",
        "valid_text = list(map(cleanpunctuation, valid_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MinNFFrIqoAl"
      },
      "outputs": [],
      "source": [
        "#tokenize an dload the dataloader\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')  # USes BPE to tokenize text sequence by merging fequently co-occured byte pair in greedy method\n",
        "tokenizer.pad_token=tokenizer.eos_token\n",
        "\n",
        "inputs_train = tokenizer(train_text, padding=True,truncation=True,max_length=args.max_seq_length) # truncate the longer sequence and pad the shorter ones\n",
        "inputs_valid=tokenizer(valid_text, padding=True,truncation=True,max_length=args.max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hATx-VPrqoAl"
      },
      "outputs": [],
      "source": [
        "# creating labels sequence for every input_ids sequence\n",
        "def create_labels(inputs):\n",
        "    labels=[]\n",
        "    for ids,attention_mask in zip(inputs['input_ids'],inputs['attention_mask']):\n",
        "        label=ids.copy()\n",
        "        real_len=sum(attention_mask)\n",
        "        padding_len=len(attention_mask)-sum(attention_mask)\n",
        "        label[:]=label[:real_len]+[-100]*padding_len  # rule out padding tokens by setting it to -100 (to avoid compute loss)\n",
        "        labels.append(label)    # automatically shifts the labels to the right to match the inputs_ids\n",
        "    inputs['labels']=labels\n",
        "\n",
        "create_labels(inputs_train)\n",
        "create_labels(inputs_valid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IieqvkDRqoAm"
      },
      "outputs": [],
      "source": [
        "class StoryDataset:\n",
        "    def __init__(self, inputs):\n",
        "        self.ids = inputs['input_ids']\n",
        "        self.attention_mask = inputs['attention_mask']\n",
        "        self.labels=inputs['labels']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)  # total number of samples in the dataset\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "\n",
        "        return [torch.tensor(self.ids[item], dtype=torch.long),\n",
        "                torch.tensor(self.attention_mask[item], dtype=torch.long),\n",
        "                torch.tensor(self.labels[item], dtype=torch.long)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gjekmb9qoAn"
      },
      "outputs": [],
      "source": [
        "# train dataset\n",
        "train_batch_size=args.train_batch_size\n",
        "valid_batch_size=args.valid_batch_size\n",
        "traindata=StoryDataset(inputs_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    traindata,\n",
        "    shuffle=False,\n",
        "    batch_size=train_batch_size)\n",
        "\n",
        "# vaidation dataset\n",
        "validdata=StoryDataset(inputs_valid)\n",
        "valid_dataloader = torch.utils.data.DataLoader(\n",
        "    validdata,\n",
        "    shuffle=False,\n",
        "    batch_size=valid_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb1VBpUjqoAn"
      },
      "outputs": [],
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model in Zero-Shot Setting (W/o Fine-tuning) and calculate Perplexity"
      ],
      "metadata": {
        "id": "F56wdl74LBvA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-b3C1jTqoAo"
      },
      "outputs": [],
      "source": [
        "model.to('cuda')   # Use GPU\n",
        "model.eval()\n",
        "eval_loss=[]\n",
        "for inputs in tqdm(valid_dataloader, desc=\"eval\"):\n",
        "    d1,d2,d3=inputs\n",
        "    d1=d1.to('cuda')\n",
        "    d2=d2.to('cuda')\n",
        "    d3=d3.to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids=d1, attention_mask=d2,labels=d3)\n",
        "        batch_loss=output[0]\n",
        "    eval_loss+=[batch_loss.cpu().item()]\n",
        "    del batch_loss\n",
        "eval_loss=np.mean(eval_loss)    # Evaluate model in zero-shot setting on validation set and calculate perplexity\n",
        "perplexity=math.exp(eval_loss)\n",
        "print(f'The average perplexity for valid dataset before fine-tuning is {perplexity}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# START: COPIED FROM <emily2008/fine-tune-gpt-2-to-generate-stories>\n",
        "# Using generate function from the model\n",
        "def generate_story(prompt, k=0, p=0.7, output_length=500, temperature=1, num_return_sequences=1, repetition_penalty=1.0):\n",
        "    print(\"----prompt----\\n\")\n",
        "    print(prompt + \"\\n\")\n",
        "\n",
        "    encoded_prompt = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "    model.to('cpu')\n",
        "    model.eval()\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=encoded_prompt,\n",
        "        max_length=output_length,\n",
        "        temperature=temperature, # control next token probability\n",
        "        top_k=k,  # number of highest probability vocabulary tokens to keep for top-k-filtering\n",
        "        top_p=p, # cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling\n",
        "        repetition_penalty=repetition_penalty, # Between 1.0 and infinity. 1.0 means no penalty\n",
        "        do_sample=True,  # if set to False greedy decoding is used\n",
        "        num_return_sequences=num_return_sequences\n",
        "    )\n",
        "\n",
        "    if len(output_sequences.shape) > 2:\n",
        "        output_sequences.squeeze_()\n",
        "\n",
        "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "        print(\"---- STORY {} ----\".format(generated_sequence_idx + 1))\n",
        "        generated_sequence = generated_sequence.tolist()\n",
        "        # Decode text\n",
        "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "        # Remove all text after eos token\n",
        "        text = text[: text.find(tokenizer.eos_token)]\n",
        "        print(text)"
      ],
      "metadata": {
        "id": "fN7uXwD3_7iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tuning the Model"
      ],
      "metadata": {
        "id": "-Oz_muUQMZxw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW85J0OgqoAp"
      },
      "outputs": [],
      "source": [
        "# number of training samples = 15620\n",
        "\n",
        "num_train_epochs = args.num_train_epochs\n",
        "training_steps_per_epoch=len(train_dataloader)\n",
        "total_num_training_steps = int(training_steps_per_epoch*num_train_epochs)\n",
        "weight_decay=0\n",
        "learning_rate=args.learning_rate\n",
        "adam_epsilon=1e-8\n",
        "warmup_steps=int(total_num_training_steps*args.warmup)\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [\n",
        "    {\n",
        "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": weight_decay,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_num_training_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cofJWChCqoAp"
      },
      "outputs": [],
      "source": [
        "#Train the model on GPU\n",
        "print(\"  Num Epochs = {}\".format(num_train_epochs))\n",
        "print(f\"  Train_batch_size per device = {train_batch_size}\")\n",
        "print(f\"  Valid_batch_size per device = {valid_batch_size}\")\n",
        "model.to('cuda')\n",
        "for epoch in range(num_train_epochs):\n",
        "    print(f\"Start epoch{epoch+1} of {num_train_epochs}\")\n",
        "    train_loss=0\n",
        "    epoch_iterator = tqdm(train_dataloader,desc='Iteration')\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    for _, inputs in enumerate(epoch_iterator):\n",
        "        d1,d2,d3=inputs\n",
        "        d1=d1.to('cuda')\n",
        "        d2=d2.to('cuda')\n",
        "        d3=d3.to('cuda')\n",
        "        output = model(input_ids=d1, attention_mask=d2,labels=d3)\n",
        "        batch_loss=output[0]\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        model.zero_grad()\n",
        "        train_loss+=batch_loss.item()\n",
        "        epoch_iterator.set_description('(batch loss=%g)' % batch_loss.item())\n",
        "        del batch_loss\n",
        "    print(f'Average train loss per example={train_loss/training_steps_per_epoch} in epoch{epoch+1}')\n",
        "    print(f'Starting evaluate after epoch {epoch+1}')\n",
        "    eval_loss=[]\n",
        "    model.eval()\n",
        "    for inputs in tqdm(valid_dataloader, desc=\"eval\"):\n",
        "        d1,d2,d3=inputs\n",
        "        d1=d1.to('cuda')\n",
        "        d2=d2.to('cuda')\n",
        "        d3=d3.to('cuda')\n",
        "        with torch.no_grad():\n",
        "            output = model(input_ids=d1, attention_mask=d2,labels=d3)\n",
        "            batch_loss=output[0]\n",
        "        eval_loss+=[batch_loss.cpu().item()]\n",
        "        del batch_loss\n",
        "    eval_loss=np.mean(eval_loss)\n",
        "    perplexity=math.exp(eval_loss)\n",
        "    print(f'Average valid loss per example={eval_loss} in epoch{epoch+1}')\n",
        "    print(f'Perplextiy for valid dataset in epoch{epoch+1} is {perplexity}')\n",
        "\n",
        "# Perplexity used as the metrics to check if fine-tuning imporves the performance or not\n",
        "# END: COPIED FROM <emily2008/fine-tune-gpt-2-to-generate-stories>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Stories using Fine-tuned Model (Example)"
      ],
      "metadata": {
        "id": "Tk8aR6KpM-xR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icbhnjhsM3aa"
      },
      "outputs": [],
      "source": [
        "# Pair of three captions\n",
        "s1 = generate_story(prompt = '[start] two street signs at an intersection of emerald and university [end]')\n",
        "s2 = generate_story(prompt = '[start] a view of a stove that is built into the cabinets [end]')\n",
        "s3 = generate_story(prompt = '[start] three adults watch a child holding a toy doll [end]')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_story(prompt = 'two street signs at an intersection of emerald and university,  a view of a stove that is built into the cabinets, three adults watch a child holding a toy doll')"
      ],
      "metadata": {
        "id": "P5jRO7-pnLsL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 2385.169536,
      "end_time": "2020-08-12T20:21:42.411421",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-08-12T19:41:57.241885",
      "version": "2.1.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}