{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tune BART Model usign Writing Prompt Dataset"
      ],
      "metadata": {
        "id": "sxUJmmEFSxJm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvSyLvqXAiR-"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -U\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to combine prompts and stories\n",
        "def combinetext(prompt, story):\n",
        "    prompts = open(prompt, 'r', encoding='utf8').readlines()\n",
        "    stories = open(story, 'r', encoding='utf8').readlines()\n",
        "    assert len(prompts) == len(stories)\n",
        "    combine = []\n",
        "    for i in range(len(prompts)):\n",
        "        combine.append(prompts[i].rstrip() + ' <sep> ' + \" \".join(stories[i].split()[:300]))\n",
        "    return combine\n",
        "\n",
        "# Prprocessing the data (punctuations, etc)\n",
        "def cleanpunctuation(s):\n",
        "    for p in '!,.:;?':\n",
        "        s = s.replace(' ' + p, p)\n",
        "    s = s.replace(' ' + 'n\\'t', 'n\\'t')\n",
        "    s = s.replace(' ' + '\\'s', '\\'s')\n",
        "    s = s.replace(' ' + '\\'re', '\\'re')\n",
        "    s = s.replace(' ' + '\\'ve', '\\'ve')\n",
        "    s = s.replace(' ' + '\\'ll', '\\'ll')\n",
        "    s = s.replace(' ' + '\\'am', '\\'am')\n",
        "    s = s.replace(' ' + '\\'m', '\\'m')\n",
        "    s = s.replace(' ' + '\\' m', '\\'m')\n",
        "    s = s.replace(' ' + '\\'m', '\\'m')\n",
        "    s = s.replace(' ' + '\\' ve', '\\'ve')\n",
        "    s = s.replace(' ' + '\\' s', '\\'s')\n",
        "    s = s.replace('<newline>', '\\n')\n",
        "    return s\n",
        "\n",
        "# Combine and clean text for train and valid datasets\n",
        "train_text = combinetext('valid.wp_source', 'valid.wp_target')\n",
        "train_text = list(map(cleanpunctuation, train_text))\n",
        "\n",
        "valid_text = combinetext('test.wp_source', 'test.wp_target')\n",
        "valid_text = list(map(cleanpunctuation, valid_text))\n"
      ],
      "metadata": {
        "id": "RzhRo0xyE5s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize and load BART-base model from Hugging Face\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "model_name = \"facebook/bart-base\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "BnTXxeANAoPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize and encode the input (conactenated text prompt and story usign <SEP>)\n",
        "def tokenize_and_encode(examples):\n",
        "    inputs, labels = [], []\n",
        "    for example in examples:\n",
        "        if ' <sep> ' in example:\n",
        "            split_text = example.split(' <sep> ')\n",
        "            inputs.append(split_text[0])\n",
        "            labels.append(split_text[1])\n",
        "        else:\n",
        "            print(\"Separator not found in example:\", example)\n",
        "            # Handle the case where separator is not found\n",
        "            continue  # Skipping this example\n",
        "\n",
        "    return tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=1024), tokenizer(labels, padding=\"max_length\", truncation=True, max_length=1024)\n",
        "\n",
        "tokenized_train = tokenize_and_encode(train_text)\n",
        "tokenized_valid = tokenize_and_encode(valid_text)"
      ],
      "metadata": {
        "id": "U5sOVof4AtLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "dOsyQI3rEunn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Check GPU availability and print the GPU name\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available. Device name:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"GPU is not available.\")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=1,  # Reduced batch size\n",
        "    per_device_eval_batch_size=1,   # Reduced batch size\n",
        "    gradient_accumulation_steps=4,  # Using gradient accumulation\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,  # Enable mixed precision\n",
        ")\n",
        "\n",
        "# Move model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_valid,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "## I tried reducing the batch size, introducing gradient accumalation steps and also mixed precision training(fp16=True). But still the model seems too big to run on Colab GPU"
      ],
      "metadata": {
        "id": "B-ZNd_hgEwvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./fine_tuned_bart\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_bart\")"
      ],
      "metadata": {
        "id": "-AqKoap8A7dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Stories for an example pair of three captions"
      ],
      "metadata": {
        "id": "zGnbquJ1Tv3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_story(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=1024, num_beams=5, early_stopping=True)\n",
        "    story = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return story\n",
        "\n",
        "# Example\n",
        "prompt = \"A dragon, a castle, and a mysterious old book\"\n",
        "print(generate_story(prompt))\n"
      ],
      "metadata": {
        "id": "vXVOQmmnA-0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}