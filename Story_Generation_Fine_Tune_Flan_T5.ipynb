{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tune Flan-T5 Model usign Writing Prompt Dataset"
      ],
      "metadata": {
        "id": "tBpURl6xUOTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch"
      ],
      "metadata": {
        "id": "1Wl52Pi4sEoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "hrPW84q7sHdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to combine prompts and stories\n",
        "def combinetext(prompt, story):\n",
        "    prompts = open(prompt, 'r', encoding='utf8').readlines()\n",
        "    stories = open(story, 'r', encoding='utf8').readlines()\n",
        "    assert len(prompts) == len(stories)\n",
        "    combine = []\n",
        "    for i in range(len(prompts)):\n",
        "        combine.append(prompts[i].rstrip() + ' <sep> ' + \" \".join(stories[i].split()[:300]))\n",
        "    return combine\n",
        "\n",
        "# Prprocessing the data (punctuations, etc)\n",
        "def cleanpunctuation(s):\n",
        "    for p in '!,.:;?':\n",
        "        s = s.replace(' ' + p, p)\n",
        "    s = s.replace(' ' + 'n\\'t', 'n\\'t')\n",
        "    s = s.replace(' ' + '\\'s', '\\'s')\n",
        "    s = s.replace(' ' + '\\'re', '\\'re')\n",
        "    s = s.replace(' ' + '\\'ve', '\\'ve')\n",
        "    s = s.replace(' ' + '\\'ll', '\\'ll')\n",
        "    s = s.replace(' ' + '\\'am', '\\'am')\n",
        "    s = s.replace(' ' + '\\'m', '\\'m')\n",
        "    s = s.replace(' ' + '\\' m', '\\'m')\n",
        "    s = s.replace(' ' + '\\'m', '\\'m')\n",
        "    s = s.replace(' ' + '\\' ve', '\\'ve')\n",
        "    s = s.replace(' ' + '\\' s', '\\'s')\n",
        "    s = s.replace('<newline>', '\\n')\n",
        "    return s"
      ],
      "metadata": {
        "id": "cOumvozAtXJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "tqYO3F7CuvAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "train_texts = combinetext('valid.wp_source', 'valid.wp_target')\n",
        "train_texts = list(map(cleanpunctuation, train_texts))\n",
        "train_dataset = StoryDataset(tokenizer, train_texts)\n",
        "\n",
        "valid_texts = combinetext('test.wp_source', 'test.wp_target')\n",
        "valid_texts = list(map(cleanpunctuation, valid_texts))\n",
        "valid_dataset = StoryDataset(tokenizer, valid_texts)\n"
      ],
      "metadata": {
        "id": "UiW0lnOCteKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize and encode the input (conactenated text prompt and story usign <SEP>)\n",
        "class StoryDataset(Dataset):\n",
        "    def __init__(self, tokenizer, texts, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "        for text in texts:\n",
        "            prompt, story = text.split('<sep>')\n",
        "            tokenized_input = tokenizer(prompt, max_length=max_length, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
        "            tokenized_target = tokenizer(story, max_length=max_length, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
        "            self.inputs.append(tokenized_input)\n",
        "            self.targets.append(tokenized_target)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_ids = self.inputs[index]['input_ids'].squeeze(0)\n",
        "        attention_mask = self.inputs[index]['attention_mask'].squeeze(0)\n",
        "        target_ids = self.targets[index]['input_ids'].squeeze(0)\n",
        "        return input_ids, attention_mask, target_ids"
      ],
      "metadata": {
        "id": "A4G-ATaUUxXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load Flan-T5 base modl from Hugging Face\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")"
      ],
      "metadata": {
        "id": "NYf2-Y-Utgd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
      ],
      "metadata": {
        "id": "xkbfh_Ymth-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune train the model\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for input_ids, attention_mask, target_ids in tqdm(train_loader):\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "UJXyHgOctjso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('./fine_tuned_model')"
      ],
      "metadata": {
        "id": "98xeu6O9tm5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import cross_entropy"
      ],
      "metadata": {
        "id": "_6cdoTw8tzDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation and compute loss\n",
        "def evaluate(model, val_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(val_loader):\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            targets = {k: v.to(device) for k, v in targets.items()}\n",
        "\n",
        "            outputs = model(**inputs, labels=targets[\"input_ids\"])\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            total += 1\n",
        "\n",
        "    return total_loss / total"
      ],
      "metadata": {
        "id": "ioYetMPVt0UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss = evaluate(model, valid_loader, device)\n",
        "print(f\"Validation Loss: {val_loss}\")\n"
      ],
      "metadata": {
        "id": "uzK4wr9Tt1xB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}