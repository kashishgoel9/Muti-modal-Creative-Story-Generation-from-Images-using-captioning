{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Training a Neural Model for Image Captioning using MS COCO Dataset"
      ],
      "metadata": {
        "id": "ZgR6mbH_3G9Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AldVDvOgcpbc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import random\n",
        "import requests\n",
        "import json\n",
        "from math import sqrt\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwIzlIc1koFO",
        "outputId": "b17bd8f5-163b-47c1-8a23-3666d7b8c234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZo55VcRlgWB"
      },
      "outputs": [],
      "source": [
        "!unzip gdrive/My\\ Drive/ic_data/coco2017.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fffmL_t1QzP_",
        "outputId": "3fc0783c-c449-43d4-d01b-bfacdd9aa499"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        image  \\\n",
              "0  train2017/000000364797.jpg   \n",
              "1  train2017/000000470291.jpg   \n",
              "2  train2017/000000027626.jpg   \n",
              "3  train2017/000000393478.jpg   \n",
              "4  train2017/000000297929.jpg   \n",
              "\n",
              "                                             caption  \n",
              "0       a large bunch of bananas near a bowl of food  \n",
              "1                   a man in a suit eats a sandwich   \n",
              "2            A bedroom that has a window by the bed.  \n",
              "3  A green and yellow train going up an incline i...  \n",
              "4  a woman is looking in the mirror as she brushe...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-414bc4af-e7bf-469a-aa4f-4d80a4d013f5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train2017/000000364797.jpg</td>\n",
              "      <td>a large bunch of bananas near a bowl of food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train2017/000000470291.jpg</td>\n",
              "      <td>a man in a suit eats a sandwich</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train2017/000000027626.jpg</td>\n",
              "      <td>A bedroom that has a window by the bed.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train2017/000000393478.jpg</td>\n",
              "      <td>A green and yellow train going up an incline i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train2017/000000297929.jpg</td>\n",
              "      <td>a woman is looking in the mirror as she brushe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-414bc4af-e7bf-469a-aa4f-4d80a4d013f5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-414bc4af-e7bf-469a-aa4f-4d80a4d013f5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-414bc4af-e7bf-469a-aa4f-4d80a4d013f5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e78ed2c8-ca46-48b3-bcbc-f750e4aeb851\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e78ed2c8-ca46-48b3-bcbc-f750e4aeb851')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e78ed2c8-ca46-48b3-bcbc-f750e4aeb851 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "with open(f'annotations/captions_train2017.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "    data = data['annotations']\n",
        "\n",
        "img_cap_pairs = []\n",
        "\n",
        "for sample in data:\n",
        "    img_name = '%012d.jpg' % sample['image_id']\n",
        "    img_cap_pairs.append([img_name, sample['caption']])\n",
        "\n",
        "captions = pd.DataFrame(img_cap_pairs, columns=['image', 'caption'])\n",
        "captions['image'] = captions['image'].apply(\n",
        "    lambda x: f'train2017/{x}'\n",
        ")\n",
        "captions = captions.sample(70000)\n",
        "captions = captions.reset_index(drop=True)\n",
        "captions.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWbe_xuhFaJp"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    text = '[start] ' + text + ' [end]'\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "v_ouwWhKnEy5",
        "outputId": "20fb2e1d-11ed-48d0-c501-fe0201d43c51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        image  \\\n",
              "0  train2017/000000364797.jpg   \n",
              "1  train2017/000000470291.jpg   \n",
              "2  train2017/000000027626.jpg   \n",
              "3  train2017/000000393478.jpg   \n",
              "4  train2017/000000297929.jpg   \n",
              "\n",
              "                                             caption  \n",
              "0  [start] a large bunch of bananas near a bowl o...  \n",
              "1      [start] a man in a suit eats a sandwich [end]  \n",
              "2  [start] a bedroom that has a window by the bed...  \n",
              "3  [start] a green and yellow train going up an i...  \n",
              "4  [start] a woman is looking in the mirror as sh...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-10bf5664-cc0a-49ab-8c21-858c1a19e194\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train2017/000000364797.jpg</td>\n",
              "      <td>[start] a large bunch of bananas near a bowl o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train2017/000000470291.jpg</td>\n",
              "      <td>[start] a man in a suit eats a sandwich [end]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train2017/000000027626.jpg</td>\n",
              "      <td>[start] a bedroom that has a window by the bed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train2017/000000393478.jpg</td>\n",
              "      <td>[start] a green and yellow train going up an i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train2017/000000297929.jpg</td>\n",
              "      <td>[start] a woman is looking in the mirror as sh...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-10bf5664-cc0a-49ab-8c21-858c1a19e194')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-10bf5664-cc0a-49ab-8c21-858c1a19e194 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-10bf5664-cc0a-49ab-8c21-858c1a19e194');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fbf34ede-25a3-4f8c-89c8-98930b099f64\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fbf34ede-25a3-4f8c-89c8-98930b099f64')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fbf34ede-25a3-4f8c-89c8-98930b099f64 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "captions['caption'] = captions['caption'].apply(preprocess)\n",
        "captions.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrpV1r9E2XJT",
        "outputId": "c1012dae-fd62-498e-e91e-4ba2617a6bf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Caption 1: [start] a large bunch of bananas near a bowl of food [end]\n",
            "Original Caption 2: [start] a man in a suit eats a sandwich [end]\n",
            "Original Caption 3: [start] a bedroom that has a window by the bed [end]\n",
            "Original Caption 4: [start] a green and yellow train going up an incline in the snow [end]\n",
            "Original Caption 5: [start] a woman is looking in the mirror as she brushes her teeth [end]\n"
          ]
        }
      ],
      "source": [
        "original_captions = [] # Create an empty list to store original captions\n",
        "\n",
        "for caption in captions['caption']: # Loop through all captions in the dataset\n",
        "    original_captions.append(caption)\n",
        "\n",
        "for i, caption in enumerate(original_captions[:5]):  # Print the first 5 captions as a sample\n",
        "    print(f\"Original Caption {i + 1}: {caption}\")\n",
        "\n",
        "# Can access all original captions in the 'original_captions' list (Example, when calculating the BLUE Score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSTivH_FSSf2"
      },
      "outputs": [],
      "source": [
        "# Defining Parameters\n",
        "MAX_LENGTH = 40\n",
        "VOCABULARY_SIZE = 15000\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "EMBEDDING_DIM = 512\n",
        "UNITS = 512\n",
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8MGUNtBN2sz"
      },
      "outputs": [],
      "source": [
        "# Defining Tokenizer\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCABULARY_SIZE,\n",
        "    standardize=None,\n",
        "    output_sequence_length=MAX_LENGTH)\n",
        "\n",
        "tokenizer.adapt(captions['caption'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "net1-mr9QzQB"
      },
      "outputs": [],
      "source": [
        "tokenizer.vocabulary_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSKV3trQQzQB"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(tokenizer.get_vocabulary(), open('vocab_coco.file', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvhg-6eKN3nz"
      },
      "outputs": [],
      "source": [
        "# START: COPIED FROM <image-captioning-on-coco-dataset>\n",
        "\n",
        "word2idx = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\", # Specify a mask token for sequence padding.\n",
        "    vocabulary=tokenizer.get_vocabulary()) # Provide the vocabulary for indexing.\n",
        "\n",
        "idx2word = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\", # Specify a mask token for sequence padding.\n",
        "    vocabulary=tokenizer.get_vocabulary(),\n",
        "    invert=True) # Invert the lookup to convert indices back to words."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a defaultdict to map images to their corresponding captions.\n",
        "img_to_cap_vector = collections.defaultdict(list)\n",
        "\n",
        "# Iterate through image-caption pairs and populate the defaultdict.\n",
        "for img, cap in zip(captions['image'], captions['caption']):\n",
        "    img_to_cap_vector[img].append(cap)\n",
        "\n",
        "# Get a list of unique image keys.\n",
        "img_keys = list(img_to_cap_vector.keys())\n",
        "random.shuffle(img_keys)\n",
        "\n",
        "# Split the image keys into training and validation sets (80% - 20% split).\n",
        "slice_index = int(len(img_keys) * 0.8)\n",
        "img_name_train_keys, img_name_val_keys = (img_keys[:slice_index], img_keys[slice_index:])\n",
        "\n",
        "train_imgs = []\n",
        "train_captions = []\n",
        "\n",
        "# Preparing the training images and captions\n",
        "for imgt in img_name_train_keys:\n",
        "    capt_len = len(img_to_cap_vector[imgt])\n",
        "    train_imgs.extend([imgt] * capt_len)\n",
        "    train_captions.extend(img_to_cap_vector[imgt])\n",
        "\n",
        "val_imgs = []\n",
        "val_captions = []\n",
        "\n",
        "# Preparing the validation set by images and captions\n",
        "for imgv in img_name_val_keys:\n",
        "    capv_len = len(img_to_cap_vector[imgv])\n",
        "    val_imgs.extend([imgv] * capv_len)\n",
        "    val_captions.extend(img_to_cap_vector[imgv])\n"
      ],
      "metadata": {
        "id": "vMtrZbMhxijy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12c-7FHzOFSq"
      },
      "outputs": [],
      "source": [
        "# Loading the data\n",
        "def load_data(img_path, caption):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    caption = tokenizer(caption)\n",
        "    return img, caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHk83y3eOFPz"
      },
      "outputs": [],
      "source": [
        "# Train Dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_imgs, train_captions))\n",
        "\n",
        "train_dataset = train_dataset.map(\n",
        "    load_data, num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# Validation dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (val_imgs, val_captions))\n",
        "\n",
        "val_dataset = val_dataset.map(\n",
        "    load_data, num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQr_bgk11eMF"
      },
      "outputs": [],
      "source": [
        "image_augmentation = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.RandomFlip(\"horizontal\"), # Randomly flip images horizontally.\n",
        "        tf.keras.layers.RandomRotation(0.2), # Randomly rotate images by up to 20%.\n",
        "        tf.keras.layers.RandomContrast(0.3), # Apply random contrast adjustment.\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9GDJ9_1nIMO"
      },
      "outputs": [],
      "source": [
        "#CNN encoder model based on InceptionV3\n",
        "def CNN_Encoder():\n",
        "  # Load the InceptionV3 pre-trained model without the top classification layer.\n",
        "    inception_v3 = tf.keras.applications.InceptionV3(\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "\n",
        "    output = inception_v3.output  # reshape the output from the InceptionV3 model.\n",
        "    output = tf.keras.layers.Reshape(\n",
        "        (-1, output.shape[-1]))(output)\n",
        "\n",
        "    cnn_model = tf.keras.models.Model(inception_v3.input, output) # Create a custom CNN model with the modified InceptionV3 base\n",
        "    return cnn_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMy5MrE2PdHV"
      },
      "outputs": [],
      "source": [
        "# A custom Transformer Encoder Layer\n",
        "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = tf.keras.layers.LayerNormalization() # Layer normalization for the first sub-layer (self-attention)\n",
        "        self.layer_norm_2 = tf.keras.layers.LayerNormalization() # second sub-layer (feed-forward network)\n",
        "        # Multi-Head Self-Attention Mechanism\n",
        "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense = tf.keras.layers.Dense(embed_dim, activation=\"relu\")\n",
        "\n",
        "\n",
        "    def call(self, x, training):\n",
        "        x = self.layer_norm_1(x) # Apply normalization to the input layer\n",
        "        x = self.dense(x) # Pass the input through a feed-forward neural network\n",
        "\n",
        "        attn_output = self.attention(\n",
        "            query=x,\n",
        "            value=x,\n",
        "            key=x,\n",
        "            attention_mask=None,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        x = self.layer_norm_2(x + attn_output) # Add the residual connection and apply layer normalization again\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFqNFts0duGB"
      },
      "outputs": [],
      "source": [
        "# combining token and positional embeddings\n",
        "class Embeddings(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, max_len):\n",
        "        super().__init__()\n",
        "        # Token embeddings layer (word embeddings)\n",
        "        self.token_embeddings = tf.keras.layers.Embedding(\n",
        "            vocab_size, embed_dim)\n",
        "        # Positional embeddings layer (to encode position information)\n",
        "        self.position_embeddings = tf.keras.layers.Embedding(\n",
        "            max_len, embed_dim, input_shape=(None, max_len))\n",
        "\n",
        "\n",
        "    def call(self, input_ids):\n",
        "        length = tf.shape(input_ids)[-1]\n",
        "        position_ids = tf.range(start=0, limit=length, delta=1)\n",
        "        position_ids = tf.expand_dims(position_ids, axis=0) # Generate positional embeddings based on the input sequence length\n",
        "\n",
        "        token_embeddings = self.token_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "        return token_embeddings + position_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcbCQqrDnJ4-"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, embed_dim, units, num_heads):\n",
        "        super().__init__()\n",
        "        self.embedding = Embeddings(\n",
        "            tokenizer.vocabulary_size(), embed_dim, MAX_LENGTH)\n",
        "\n",
        "        self.attention_1 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "\n",
        "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.ffn_layer_1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
        "        self.ffn_layer_2 = tf.keras.layers.Dense(embed_dim)\n",
        "\n",
        "        self.out = tf.keras.layers.Dense(tokenizer.vocabulary_size(), activation=\"softmax\")\n",
        "\n",
        "        # Dropout layers for regularization\n",
        "        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n",
        "        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "\n",
        "    def call(self, input_ids, encoder_output, training, mask=None):\n",
        "        embeddings = self.embedding(input_ids)\n",
        "\n",
        "        combined_mask = None\n",
        "        padding_mask = None\n",
        "\n",
        "        if mask is not None:\n",
        "            causal_mask = self.get_causal_attention_mask(embeddings)\n",
        "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
        "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
        "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
        "\n",
        "        attn_output_1 = self.attention_1(\n",
        "            query=embeddings,\n",
        "            value=embeddings,\n",
        "            key=embeddings,\n",
        "            attention_mask=combined_mask,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        out_1 = self.layernorm_1(embeddings + attn_output_1) # Dropout layers for regularization\n",
        "\n",
        "        # Second self-attention layer (cross-attention with encoder_output)\n",
        "        attn_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_output,\n",
        "            key=encoder_output,\n",
        "            attention_mask=padding_mask,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        out_2 = self.layernorm_2(out_1 + attn_output_2)\n",
        "\n",
        "        ffn_out = self.ffn_layer_1(out_2)\n",
        "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
        "        ffn_out = self.ffn_layer_2(ffn_out)\n",
        "\n",
        "        ffn_out = self.layernorm_3(ffn_out + out_2)\n",
        "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
        "        preds = self.out(ffn_out)\n",
        "        return preds\n",
        "\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "      # Generate a causal (triangular) attention mask for self-attention\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_NmSUaVys9R"
      },
      "outputs": [],
      "source": [
        "class ImageCaptioningModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, cnn_model, encoder, decoder, image_aug=None):\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.image_aug = image_aug\n",
        "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "        self.acc_tracker = tf.keras.metrics.Mean(name=\"accuracy\")\n",
        "\n",
        "\n",
        "    def calculate_loss(self, y_true, y_pred, mask):\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
        "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
        "        accuracy = tf.math.logical_and(mask, accuracy)\n",
        "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "    def compute_loss_and_acc(self, img_embed, captions, training=True):\n",
        "        encoder_output = self.encoder(img_embed, training=True)\n",
        "        y_input = captions[:, :-1]\n",
        "        y_true = captions[:, 1:]\n",
        "        mask = (y_true != 0)\n",
        "        y_pred = self.decoder(\n",
        "            y_input, encoder_output, training=True, mask=mask\n",
        "        )\n",
        "        loss = self.calculate_loss(y_true, y_pred, mask)\n",
        "        acc = self.calculate_accuracy(y_true, y_pred, mask)\n",
        "        return loss, acc\n",
        "\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        imgs, captions = batch\n",
        "\n",
        "        if self.image_aug:\n",
        "            imgs = self.image_aug(imgs)\n",
        "\n",
        "        img_embed = self.cnn_model(imgs)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss, acc = self.compute_loss_and_acc(\n",
        "                img_embed, captions\n",
        "            )\n",
        "\n",
        "        train_vars = (\n",
        "            self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "        )\n",
        "        grads = tape.gradient(loss, train_vars)\n",
        "        self.optimizer.apply_gradients(zip(grads, train_vars))\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(acc)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "\n",
        "    def test_step(self, batch):\n",
        "        imgs, captions = batch\n",
        "\n",
        "        img_embed = self.cnn_model(imgs)\n",
        "\n",
        "        loss, acc = self.compute_loss_and_acc(\n",
        "            img_embed, captions, training=False\n",
        "        )\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(acc)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.acc_tracker]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqWpcsje0Hkh"
      },
      "outputs": [],
      "source": [
        "encoder = TransformerEncoderLayer(EMBEDDING_DIM, 1)\n",
        "decoder = TransformerDecoderLayer(EMBEDDING_DIM, UNITS, 8)\n",
        "\n",
        "cnn_model = CNN_Encoder()\n",
        "caption_model = ImageCaptioningModel(\n",
        "    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bayNssgNX6QN"
      },
      "outputs": [],
      "source": [
        "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False, reduction=\"none\"\n",
        ")\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "caption_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=cross_entropy\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RYo-MRVYn49"
      },
      "outputs": [],
      "source": [
        "history = caption_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=1,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKLdug_LQzQE"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ErlQQICtj_g"
      },
      "outputs": [],
      "source": [
        "def load_image_from_path(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def generate_caption(img_path, add_noise=False):\n",
        "    img = load_image_from_path(img_path)\n",
        "\n",
        "    if add_noise:\n",
        "        noise = tf.random.normal(img.shape)*0.1\n",
        "        img = img + noise\n",
        "        img = (img - tf.reduce_min(img))/(tf.reduce_max(img) - tf.reduce_min(img))\n",
        "\n",
        "    img = tf.expand_dims(img, axis=0) # Expand the image dimensions to match the model input shape\n",
        "    img_embed = caption_model.cnn_model(img) # Encode the image using the CNN model\n",
        "    img_encoded = caption_model.encoder(img_embed, training=False)\n",
        "\n",
        "    y_inp = '[start]' # Initialize the caption with a start token\n",
        "\n",
        "    # Generate the caption one word at a time\n",
        "    for i in range(MAX_LENGTH-1):\n",
        "      # Tokenize the current caption and create a mask\n",
        "        tokenized = tokenizer([y_inp])[:, :-1]\n",
        "        mask = tf.cast(tokenized != 0, tf.int32)\n",
        "\n",
        "        # Predict the next word using the decoder\n",
        "        pred = caption_model.decoder(\n",
        "            tokenized, img_encoded, training=False, mask=mask)\n",
        "\n",
        "        # Get the index of the most likely next word\n",
        "        pred_idx = np.argmax(pred[0, i, :])\n",
        "        pred_idx = tf.convert_to_tensor(pred_idx)\n",
        "        pred_word = idx2word(pred_idx).numpy().decode('utf-8') # Convert the predicted index to a word and append it to the caption\n",
        "\n",
        "        # Check if the generated word is the end token\n",
        "        if pred_word == '[end]':\n",
        "            break\n",
        "\n",
        "        y_inp += ' ' + pred_word # # Append the predicted word to the caption\n",
        "\n",
        "    y_inp = y_inp.replace('[start] ', '') # Remove the start token from the final caption\n",
        "    return y_inp\n",
        "# END: COPIED FROM <image-captioning-on-coco-dataset>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27_bJe_M1Drr"
      },
      "outputs": [],
      "source": [
        "# test witha  random image from the dataet\n",
        "idx = random.randrange(0, len(captions))\n",
        "img_path = captions.iloc[idx].image\n",
        "\n",
        "pred_caption = generate_caption(img_path)\n",
        "print('Predicted Caption:', pred_caption)\n",
        "print()\n",
        "Image.open(img_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Dataset - Flickr8k"
      ],
      "metadata": {
        "id": "eONTOQWL3A4e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rs61AVgNOIgZ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RW0KFlVuQtgD"
      },
      "outputs": [],
      "source": [
        "!unzip gdrive/My\\ Drive/ic_data/archive.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i2zNUGzRGCI"
      },
      "outputs": [],
      "source": [
        "image_path = 'Images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ECgPIxPRIG9"
      },
      "outputs": [],
      "source": [
        "flickr8k_data = pd.read_csv(\"captions.txt\")\n",
        "flickr8k_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynyE8tg6OPr3"
      },
      "outputs": [],
      "source": [
        "flickr8k_data['caption'] = flickr8k_data['caption'].apply(preprocess)\n",
        "flickr8k_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "363JYIlzdm9O"
      },
      "outputs": [],
      "source": [
        "tokenizer_test = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCABULARY_SIZE,\n",
        "    standardize=None,\n",
        "    output_sequence_length=MAX_LENGTH)\n",
        "\n",
        "tokenizer_test.adapt(flickr8k_data['caption'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZkZM55AdxiW"
      },
      "outputs": [],
      "source": [
        "tokenizer_test.vocabulary_size()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(tokenizer.get_vocabulary(), open('vocab_flickr8k', 'wb'))"
      ],
      "metadata": {
        "id": "3FZsUr3Fz_5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\", # Specify a mask token for sequence padding.\n",
        "    vocabulary=tokenizer.get_vocabulary()) # Provide the vocabulary for indexing.\n",
        "\n",
        "idx2word = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\", # Specify a mask token for sequence padding.\n",
        "    vocabulary=tokenizer.get_vocabulary(),\n",
        "    invert=True) # Invert the lookup to convert indices back to words."
      ],
      "metadata": {
        "id": "iyCdbUGUz4TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nlo_a8wUedfu"
      },
      "outputs": [],
      "source": [
        "img_to_cap_vector_test = collections.defaultdict(list)\n",
        "for img, cap in zip(flickr8k_data['image'], flickr8k_data['caption']):\n",
        "    img_to_cap_vector_test[img].append(cap)\n",
        "\n",
        "img_test_keys = list(img_to_cap_vector_test.keys())\n",
        "# random.shuffle(img_test_keys)\n",
        "\n",
        "test_imgs = []\n",
        "test_captions = []\n",
        "for imgt in img_test_keys:\n",
        "    capt_len = len(img_to_cap_vector_test[imgt])\n",
        "    test_imgs.extend([imgt] * capt_len)\n",
        "    test_captions.extend(img_to_cap_vector_test[imgt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SosW2ffJfRO9"
      },
      "outputs": [],
      "source": [
        "def load_data(img_path, caption):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    caption = tokenizer(caption)\n",
        "    return img, caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKwC-J9pfVyG"
      },
      "outputs": [],
      "source": [
        "from IPython.testing import test\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_imgs, test_captions))\n",
        "\n",
        "test_dataset = test_dataset.map(\n",
        "    load_data, num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLh323Hrflgg"
      },
      "outputs": [],
      "source": [
        "print(test_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I92u3KkAZS3j"
      },
      "outputs": [],
      "source": [
        "print(flickr8k_data.iloc[0].image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate captiosn usign the generate_caption method (neural model trained usign MS COCO dataset)"
      ],
      "metadata": {
        "id": "o7o2uREk3sKu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xdQmsFVOV6P"
      },
      "outputs": [],
      "source": [
        "original_captions = []\n",
        "predicted_captions = []\n",
        "\n",
        "# Loop through the test dataset and generate the captions\n",
        "for idx, row in flickr8k_data.iterrows():\n",
        "    img_path = flickr8k_data.iloc[idx].image\n",
        "    original_caption = row['caption']\n",
        "    pred_caption = generate_caption('Images/' + img_path) # Generate captiosn usign the generate_caption method (neural model trained usign MS COCO dataset)\n",
        "\n",
        "    # Append original (from the train dataset) and predicted captions to the lists\n",
        "    original_captions.append(original_caption)\n",
        "    predicted_captions.append(pred_caption)\n",
        "\n",
        "    print('Original Caption:', original_caption)\n",
        "    print('Predicted Caption:', pred_caption)\n",
        "    print()\n",
        "    Image.open('Images/' + img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlM2pKfwOb9N"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# Convert the original captions into the required format for BLEU score computation\n",
        "original_captions = [[caption.split()] for caption in original_captions]\n",
        "\n",
        "# Compute the BLEU score\n",
        "bleu_score = corpus_bleu(original_captions, predicted_captions)\n",
        "print('BLEU Score:', bleu_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjqn39LKQzQF"
      },
      "outputs": [],
      "source": [
        "caption_model.save_weights('model.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}